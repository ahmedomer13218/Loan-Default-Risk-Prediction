{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26aa3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e78158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:11:01 WARN Utils: Your hostname, m-Laptop resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp3s0)\n",
      "25/04/28 20:11:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 20:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/28 20:11:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"nb_map_reduce\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f782040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# relative paths\n",
    "current_dir = os.getcwd()\n",
    "data_relative_path = os.path.join('..', 'splitted_data')\n",
    "\n",
    "# read the data\n",
    "x_train = spark.read.csv(os.path.join(current_dir, data_relative_path, 'X_train.csv'), header=True, inferSchema=True)\n",
    "x_test = spark.read.csv(os.path.join(current_dir, data_relative_path, 'X_test.csv'), header=True, inferSchema=True)\n",
    "y_train = spark.read.csv(os.path.join(current_dir, data_relative_path, 'y_train.csv'), header=True, inferSchema=True)\n",
    "y_test = spark.read.csv(os.path.join(current_dir, data_relative_path, 'y_test.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea81c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique IDs (join label with features)\n",
    "x_train = x_train.withColumn(\"index\", monotonically_increasing_id())\n",
    "y_train = y_train.withColumn(\"index\", monotonically_increasing_id())\n",
    "x_test = x_test.withColumn(\"index\", monotonically_increasing_id())\n",
    "y_test = y_test.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "train_df = x_train.join(y_train, \"index\").drop(\"index\")\n",
    "test_df = x_test.join(y_test, \"index\").drop(\"index\")\n",
    "\n",
    "# Rename \"TARGET\" -> \"label\"\n",
    "train_df = train_df.withColumnRenamed(\"TARGET\", \"label\")\n",
    "test_df = test_df.withColumnRenamed(\"TARGET\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40f6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# merge features into one vector (all columns except label)\n",
    "feature_columns = [col for col in train_df.columns if col != \"label\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "train_features = assembler.transform(train_df).select(\"features\", \"label\")\n",
    "test_features = assembler.transform(test_df).select(\"features\", \"label\")\n",
    "\n",
    "# collect labels to list\n",
    "train_labels_list = train_features.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "test_labels_list = test_features.select(\"label\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbea115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def discretize_features(df, num_bins=10):\n",
    "\n",
    "    feature_stats = df.select(\"features\").rdd.map(lambda row: row[\"features\"]) \\\n",
    "        .aggregate((None, None),\n",
    "                #    seq op\n",
    "                    lambda acc, features: (\n",
    "                        features if acc[0] is None else [min(a, b) for a, b in zip(acc[0], features)],\n",
    "                        features if acc[1] is None else [max(a, b) for a, b in zip(acc[1], features)]\n",
    "                    ),\n",
    "                #    comb op (partitions)\n",
    "                    lambda acc1, acc2: (\n",
    "                        acc1[0] if acc2[0] is None else (acc2[0] if acc1[0] is None else [min(a, b) for a, b in zip(acc1[0], acc2[0])]),\n",
    "                        acc1[1] if acc2[1] is None else (acc2[1] if acc1[1] is None else [max(a, b) for a, b in zip(acc1[1], acc2[1])])\n",
    "                    ))\n",
    "    \n",
    "    min_values, max_values = feature_stats\n",
    "\n",
    "    def discretize_vector(vec):\n",
    "        features = vec[\"features\"]\n",
    "        discrete_features = {}\n",
    "        for i, val in enumerate(features):\n",
    "            if val is not None:\n",
    "                min_val = min_values[i]\n",
    "                max_val = max_values[i]\n",
    "                if min_val == max_val:\n",
    "                    bin_idx = 0\n",
    "                else:\n",
    "                    bin_width = (max_val - min_val) / num_bins\n",
    "                    bin_idx = int((val - min_val) / bin_width)\n",
    "                    bin_idx = min(bin_idx, num_bins - 1)\n",
    "            else:\n",
    "                bin_idx = 0\n",
    "            discrete_features[f\"feature_{i}\"] = f\"bin_{bin_idx}\"\n",
    "        return {\"features\": discrete_features, \"label\": vec[\"label\"]}\n",
    "    \n",
    "    return df.rdd.map(discretize_vector)\n",
    "\n",
    "\n",
    "# Discretize continuous features\n",
    "train_discrete = discretize_features(train_features)\n",
    "test_discrete = discretize_features(test_features)\n",
    "\n",
    "# calc prior prob (map/reduce) ex: [(\"1\", 1), (\"1\", 1), (\"0\", 1)] => [(\"1\", 2), (\"0\", 1)]\n",
    "class_counts = train_discrete.map(lambda row: (row[\"label\"], 1)) \\\n",
    "                          .reduceByKey(lambda a, b: a + b) \\\n",
    "                          .collect()\n",
    "\n",
    "# calc prior prob ( p(label) = label_count / total_count )\n",
    "total_samples = sum(count for _, count in class_counts)\n",
    "class_probabilities = {label: count / total_samples for label, count in class_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192f6d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# feature_vec_count (map/reduce) ex: [((1, \"feature_0\", \"bin_3\"), 1), ((1, \"feature_1\", \"bin_5\"), 1)] => sum by key\n",
    "feature_counts_rdd = train_discrete.flatMap(lambda row: [\n",
    "    ((row[\"label\"], feature_name, feature_value), 1) \n",
    "    for feature_name, feature_value in row[\"features\"].items()\n",
    "]) \\\n",
    ".reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "feature_vec_count = feature_counts_rdd.collect()\n",
    "\n",
    "# {label -> {feature_name -> {feature_value -> {count}}}}\n",
    "feature_counts_dict = {}\n",
    "for (label, feature_name, feature_value), count in feature_vec_count:\n",
    "    if label not in feature_counts_dict:\n",
    "        feature_counts_dict[label] = {}\n",
    "    if feature_name not in feature_counts_dict[label]:\n",
    "        feature_counts_dict[label][feature_name] = {}\n",
    "    feature_counts_dict[label][feature_name][feature_value] = count\n",
    "\n",
    "# for each feature, get all its unique values\n",
    "all_feature_values = {}\n",
    "for label in feature_counts_dict:\n",
    "    for feature_name in feature_counts_dict[label]:\n",
    "        if feature_name not in all_feature_values:\n",
    "            all_feature_values[feature_name] = set()\n",
    "        all_feature_values[feature_name].update(feature_counts_dict[label][feature_name].keys())\n",
    "\n",
    "# feature prob (likelihood) - Laplace smoothing\n",
    "# p(feature_value | label) = (count(label, feature name, feature value) + 1) / (count(label) + unique_values)\n",
    "feature_probabilities = {}\n",
    "for label in feature_counts_dict:\n",
    "    feature_probabilities[label] = {}\n",
    "    for feature_name in feature_counts_dict[label]:\n",
    "        feature_probabilities[label][feature_name] = {}\n",
    "        total_count = sum(feature_counts_dict[label][feature_name].values())\n",
    "        unique_values = len(all_feature_values[feature_name])\n",
    "        \n",
    "        for feature_value in all_feature_values[feature_name]:\n",
    "            count = feature_counts_dict[label][feature_name].get(feature_value, 0)\n",
    "            feature_probabilities[label][feature_name][feature_value] = (count + 1) / (total_count + unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407604e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast to workers\n",
    "feature_probs_broadcast = spark.sparkContext.broadcast(feature_probabilities)\n",
    "class_probs_broadcast = spark.sparkContext.broadcast(class_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a668c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.91\n",
      "Testing Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Prediction function (map)\n",
    "def predict_sample(vec):\n",
    "    features = vec[\"features\"]\n",
    "    feature_probs = feature_probs_broadcast.value\n",
    "    class_probs = class_probs_broadcast.value\n",
    "    \n",
    "    # calc log prob for each class\n",
    "    log_probs = {}\n",
    "    for label in class_probs:\n",
    "        log_prob = math.log(class_probs[label]) # prior prob\n",
    "        # feature probs\n",
    "        for feature_name, feature_value in features.items():\n",
    "            if (label in feature_probs and \n",
    "                feature_name in feature_probs[label] and \n",
    "                feature_value in feature_probs[label][feature_name]):\n",
    "                prob = feature_probs[label][feature_name][feature_value]\n",
    "            else: \n",
    "                # in case of unseen feature/value (take small prob)\n",
    "                prob = 1e-10 \n",
    "            log_prob += math.log(max(prob, 1e-10)) # avoid log(0)\n",
    "        log_probs[label] = log_prob\n",
    "    \n",
    "    # get the class that has higher prob (reduce)\n",
    "    predicted_label = max(log_probs, key=log_probs.get)\n",
    "    return predicted_label\n",
    "\n",
    "# collect predictions (map)\n",
    "test_predictions = test_discrete.map(predict_sample).collect()\n",
    "train_predictions = train_discrete.map(predict_sample).collect()\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(train_labels_list, train_predictions)\n",
    "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
