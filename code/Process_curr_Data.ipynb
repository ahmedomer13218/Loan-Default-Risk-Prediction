{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up PySpark"
      ],
      "metadata": {
        "id": "6GDOHp3GNRym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkFWDl6AysmT",
        "outputId": "4e36883a-af8a-48f2-e419-2302335e784a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "aZlRk9950rH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf"
      ],
      "metadata": {
        "id": "39lD5twf0xS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "zqoZyrc2NV_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"Process_curr_Data\")\\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fP3gEZHo0zDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acVdIjW_02Rt",
        "outputId": "4bebea31-2896-4dec-d0c2-f016b293f50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/drive/MyDrive/Data/application_data.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlaP_bsv04X8",
        "outputId": "98d80e09-f946-427f-a412-39b4f439c56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /content/drive/MyDrive/Data/application_data.csv.zip: Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Data/application_data.csv.zip\" -d \"/content/current_application.csv\""
      ],
      "metadata": {
        "id": "pMIh98Z41tKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_curr = spark.read.csv(\"/content/current_application.csv/application_data.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "Lkn2zWzS13WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data -> col with Threshold for null & row contains nulls"
      ],
      "metadata": {
        "id": "nidc1i7yNbSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "from pyspark.sql.functions import collect_set\n",
        "from pyspark.sql.functions import col, count, when"
      ],
      "metadata": {
        "id": "sDCrkkMK17xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df, null_threshold=25):\n",
        "    # Get null percentage in each column\n",
        "    total_rows = df.count()\n",
        "    null_perc = df.select([\n",
        "    ( (count(when(col(c).isNull(), c)) / total_rows) * 100 ).alias(c)\n",
        "    for c in df.columns\n",
        "    ])\n",
        "\n",
        "    null_perc_row = null_perc.collect()[0].asDict()\n",
        "\n",
        "    # Decide which columns to drop - threshold\n",
        "    cols_to_drop = [col_name for col_name, perc in null_perc_row.items() if perc > null_threshold]\n",
        "    print(f\"Columns to drop:  {cols_to_drop}\")\n",
        "\n",
        "    # Drop columns with nulls over threshold\n",
        "    data_cleaned = df.drop(*cols_to_drop)\n",
        "    print(f\"Shape after dropping cols: ({data_cleaned.count()}, {len(data_cleaned.columns)})\")\n",
        "\n",
        "    # Drop rows with any nulls\n",
        "    data_cleaned = data_cleaned.dropna()\n",
        "    print(f\"Shape after dropping rows: ({data_cleaned.count()}, {len(data_cleaned.columns)})\")\n",
        "\n",
        "    return data_cleaned"
      ],
      "metadata": {
        "id": "Ym1iNE592Gd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_curr = clean_data(df_curr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpRpVkIu214f",
        "outputId": "cfbe038b-582d-478b-940f-f561a0691fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns to drop:  ['OWN_CAR_AGE', 'OCCUPATION_TYPE', 'EXT_SOURCE_1', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
            "Shape after dropping cols: (307511, 72)\n",
            "Shape after dropping rows: (244280, 72)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Cleaned Data"
      ],
      "metadata": {
        "id": "gpQBMpsJNoY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "mTG6l7Lr53-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your paths\n",
        "current_dir = '/content/drive/MyDrive'\n",
        "data_relative_path = 'Data'\n",
        "output_dir = os.path.join(current_dir, data_relative_path, \"tmp_output\")\n",
        "\n",
        "def save_csv(df, final_csv_path):\n",
        "    # write df into a temporary folder\n",
        "    df.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_dir)\n",
        "\n",
        "    # find the generated part file\n",
        "    part_file = next(Path(output_dir).glob(\"part-*.csv\"))\n",
        "\n",
        "    # move and rename\n",
        "    shutil.move(str(part_file), final_csv_path)\n",
        "\n",
        "    # delete the temporary folder\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "    return f\"File saved to: {final_csv_path}\"\n",
        "\n",
        "# Example usage\n",
        "final_csv_path = os.path.join(current_dir, data_relative_path, \"processed_current_application.csv\")\n",
        "save_csv(df_curr, final_csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ihZtDFXJ52gB",
        "outputId": "46553033-3067-48a6-8ffc-badef8c2d695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File saved to: /content/drive/MyDrive/Data/processed_current_application.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okMc9K5-357Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}